{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqtyFZlwtYuabwTXXkH+Gc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiaSinghania/NOAA_NDVI_Forecasting_IDL/blob/main/Midterm_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimum code that lets you download a single file - should be enough to start parsing the basic data format."
      ],
      "metadata": {
        "id": "Q3vGOuDPV7xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### MINIMUM DOWNLOAD FOR SINGLE FILE ###\n",
        "# can change that url to any file url from https://noaa-cdr-hydrological-properties-pds.s3.amazonaws.com/index.html\n",
        "!aws s3 cp s3://noaa-cdr-hydrological-properties-pds/data/ . --no-sign-request --region us-east-1\n",
        "\n",
        "import xarray as xr\n",
        "\n",
        "ds = xr.open_dataset(\"AOT_AVHRR_v04r00-preliminary_monthly-avg_202301_c20230418.nc\")\n",
        "print(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn1HlIgZVhkg",
        "outputId": "af8af09c-baa0-4c3a-caeb-311952d8fafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download: s3://noaa-cdr-aerosol-optical-thickness-pds/data/monthly/AOT_AVHRR_v04r00-preliminary_monthly-avg_202301_c20230418.nc to ./AOT_AVHRR_v04r00-preliminary_monthly-avg_202301_c20230418.nc\n",
            "<xarray.Dataset> Size: 26MB\n",
            "Dimensions:      (time: 1, latitude: 1800, longitude: 3600, nv: 2)\n",
            "Coordinates:\n",
            "  * time         (time) datetime64[ns] 8B 2023-01-01\n",
            "  * latitude     (latitude) float32 7kB -89.95 -89.85 -89.75 ... 89.85 89.95\n",
            "  * longitude    (longitude) float32 14kB -179.9 -179.8 -179.8 ... 179.8 179.9\n",
            "Dimensions without coordinates: nv\n",
            "Data variables:\n",
            "    aot1         (time, latitude, longitude) float32 26MB ...\n",
            "    lat_bounds   (latitude, nv) float32 14kB ...\n",
            "    lon_bounds   (longitude, nv) float32 29kB ...\n",
            "    time_bounds  (time, nv) datetime64[ns] 16B ...\n",
            "Attributes: (12/46)\n",
            "    Conventions:                CF-1.6, ACDD-1.3\n",
            "    title:                      Monthly AVHRR Aerosol Optical Thickness over ...\n",
            "    references:                 doi: 10.1002/jgrd.50278.2015 and AVHRR_AOT_CD...\n",
            "    source:                     https://doi.org/10.7289/V5X9287S\n",
            "    Metadata_Conventions:       CF-1.6, Unidata Dataset Discovery v1.0, NOAA ...\n",
            "    cdm_data_type:              Grid\n",
            "    ...                         ...\n",
            "    instrument:                 AVHRR-3 > Advanced Very High Resolution Radio...\n",
            "    platform_vocabulary:        NASA/GCMD Platform Keywords. Version 8.1\n",
            "    instrument_vocabulary:      NASA/GCMD Instrument Keywords. Version 8.1\n",
            "    spatial_resolution:         0.1 by 0.1 degree equal angle\n",
            "    PROGLANG:                   IDL\n",
            "    comment:                    Data are preliminary only. Preliminary means ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# double check that we can access the datasource + we have the valid packages to access the data\n",
        "!apt remove awscli -y\n",
        "!pip install boto3 s3fs xarray netCDF4 zarr\n",
        "!pip install awscli --upgrade\n",
        "!aws s3 ls s3://noaa-cdr-hydrological-properties-pds/ --region us-east-1 --no-sign-request | head\n",
        "!aws s3 ls s3://noaa-cdr-ndvi-pds/ --region us-east-1 --no-sign-request | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2weQi74eMfDt",
        "outputId": "4e2e59d6-90cd-4d2c-fa2a-c89c3a79986c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package 'awscli' is not installed, so not removed\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  docutils-common fonts-droid-fallback fonts-noto-mono ghostscript groff\n",
            "  gsfonts imagemagick imagemagick-6-common imagemagick-6.q16 libdjvulibre-text\n",
            "  libdjvulibre21 libfftw3-double3 libgs9 libgs9-common libidn12 libijs-0.35\n",
            "  libimagequant0 libjbig2dec0 libjxr-tools libjxr0 liblqr-1-0\n",
            "  libmagickcore-6.q16-6 libmagickcore-6.q16-6-extra libmagickwand-6.q16-6\n",
            "  libnetpbm10 libraqm0 libwmflite-0.2-7 netpbm psutils python3-botocore\n",
            "  python3-certifi python3-chardet python3-colorama python3-dateutil\n",
            "  python3-docutils python3-idna python3-jmespath python3-olefile python3-pil\n",
            "  python3-pyasn1 python3-requests python3-roman python3-rsa python3-s3transfer\n",
            "  python3-urllib3 sgml-base xml-core\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (1.40.59)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/dist-packages (0.4.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (2025.10.1)\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.12/dist-packages (1.7.3)\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.12/dist-packages (3.1.3)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.59 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.40.59)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from boto3) (0.14.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from s3fs) (2025.3.0)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from xarray) (2.0.2)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.12/dist-packages (from xarray) (2.2.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.12/dist-packages (from netCDF4) (1.6.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netCDF4) (2025.10.5)\n",
            "Requirement already satisfied: donfig>=0.8 in /usr/local/lib/python3.12/dist-packages (from zarr) (0.8.1.post1)\n",
            "Requirement already satisfied: numcodecs>=0.14 in /usr/local/lib/python3.12/dist-packages (from numcodecs[crc32c]>=0.14->zarr) (0.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.9 in /usr/local/lib/python3.12/dist-packages (from zarr) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.59->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.59->boto3) (2.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from donfig>=0.8->zarr) (6.0.3)\n",
            "Requirement already satisfied: crc32c>=2.7 in /usr/local/lib/python3.12/dist-packages (from numcodecs[crc32c]>=0.14->zarr) (2.8)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.59->boto3) (1.17.0)\n",
            "Requirement already satisfied: awscli in /usr/local/lib/python3.12/dist-packages (1.42.59)\n",
            "Requirement already satisfied: botocore==1.40.59 in /usr/local/lib/python3.12/dist-packages (from awscli) (1.40.59)\n",
            "Requirement already satisfied: docutils<=0.19,>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from awscli) (0.19)\n",
            "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from awscli) (0.14.0)\n",
            "Requirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.12/dist-packages (from awscli) (6.0.3)\n",
            "Requirement already satisfied: colorama<0.4.7,>=0.2.5 in /usr/lib/python3/dist-packages (from awscli) (0.4.4)\n",
            "Requirement already satisfied: rsa<4.8,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from awscli) (4.7.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from botocore==1.40.59->awscli) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore==1.40.59->awscli) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore==1.40.59->awscli) (2.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.40.59->awscli) (1.17.0)\n",
            "                           PRE data/\n",
            "                           PRE documentation/\n",
            "2024-12-17 19:41:25      36822 index.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE SCAFFOLDING:\n",
        "\n",
        "\"\"\"\n",
        "overall setup for code:\n",
        "DATA PROCESSING:\n",
        "1. load the data from raw noaa files into large array of chronologically sorted vectors (date, all other features in a F X N array) (save as a csv if its not too massive???)\n",
        "  - first index should store week data (i think counting up from 0 --> the last week number woul dmake sense)\n",
        "2. collcet the ndvi data from the same timeframe and append that data to our massive array\n",
        "3. convert entire training set to train/ validation/ test (first 75% is train, next 10% is validation, last 15% is train; chronologically speaking)\n",
        "  some people on the internet say the validation and test splits should overlap slightly, but i havent validated yet.\n",
        "4. batch the training set and store those batches in a dataloader for training purposes.\n",
        "\n",
        "\n",
        "MODEL TRAINING:\n",
        "1. write an LSTM class\n",
        "2.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rX1yYp6g2zwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import datetime\n",
        "import subprocess\n",
        "\n",
        "def s3_file_exists(s3_path):\n",
        "      \"\"\"Check whether an object exists in the S3 bucket (no sign-in required).\"\"\"\n",
        "      cmd = [\"aws\", \"s3\", \"ls\", s3_path, \"--no-sign-request\", \"--region\", region]\n",
        "      result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "      return result.returncode == 0 and len(result.stdout) > 0\n",
        "\n",
        "\n",
        "def download_noaa_hydrological(start_year, end_year, step_days=7):\n",
        "    bucket = \"noaa-cdr-hydrological-properties-pds\"\n",
        "    prefix = \"data\"\n",
        "    region = \"us-east-1\"\n",
        "\n",
        "    base_dir = \"data/hydrological\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    doy_pattern = re.compile(r\"D(\\d{2})(\\d{3})\")  # matches DYYDDD, e.g. D17121\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for month in range(1, 13):\n",
        "            month_str = f\"{month:02d}\"\n",
        "            month_dir = os.path.join(base_dir, f\"{year}_{month_str}\")\n",
        "            os.makedirs(month_dir, exist_ok=True)\n",
        "\n",
        "            print(f\"\\nüìÖ Listing Hydrological files for {year}-{month_str}...\")\n",
        "            cmd = [\n",
        "                \"aws\", \"s3\", \"ls\", f\"s3://{bucket}/{prefix}/{year}/{month_str}/\",\n",
        "                \"--no-sign-request\", \"--region\", region\n",
        "            ]\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "            if result.returncode != 0 or not result.stdout.strip():\n",
        "                print(f\"‚ö†Ô∏è No data for {year}-{month_str}\")\n",
        "                continue\n",
        "\n",
        "            # Extract filenames\n",
        "            files = [line.split()[-1] for line in result.stdout.splitlines() if line.endswith(\".nc\")]\n",
        "\n",
        "            # Parse approximate date from filename\n",
        "            def extract_date(fname):\n",
        "                m = doy_pattern.search(fname)\n",
        "                if not m:\n",
        "                    return None\n",
        "                yy, doy = int(m.group(1)), int(m.group(2))\n",
        "                year_full = 2000 + yy if yy < 50 else 1900 + yy  # crude century logic\n",
        "                return datetime.datetime(year_full, 1, 1) + datetime.timedelta(days=doy - 1)\n",
        "\n",
        "            file_dates = [(f, extract_date(f)) for f in files]\n",
        "            file_dates = [(f, d) for f, d in file_dates if d]\n",
        "            file_dates.sort(key=lambda x: x[1])\n",
        "\n",
        "            if not file_dates:\n",
        "                print(f\"‚ö†Ô∏è Could not extract dates for {year}-{month_str}\")\n",
        "                continue\n",
        "\n",
        "            # Sample roughly one per step_days\n",
        "            sampled = []\n",
        "            last_date = None\n",
        "            for fname, date in file_dates:\n",
        "                if not last_date or (date - last_date).days >= step_days:\n",
        "                    sampled.append(fname)\n",
        "                    last_date = date\n",
        "\n",
        "            print(f\"Found {len(files)} files, sampled {len(sampled)} (~1 per {step_days} days)\")\n",
        "\n",
        "            for fname in sampled:\n",
        "                s3_path = f\"s3://{bucket}/{prefix}/{year}/{month_str}/{fname}\"\n",
        "                local_path = os.path.join(month_dir, fname)\n",
        "\n",
        "                if os.path.exists(local_path):\n",
        "                    print(f\"‚úÖ Already downloaded: {fname}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"‚¨áÔ∏è Downloading: {fname}\")\n",
        "                subprocess.run([\n",
        "                    \"aws\", \"s3\", \"cp\", s3_path, local_path,\n",
        "                    \"--no-sign-request\", \"--region\", region\n",
        "                ])\n",
        "\n",
        "    print(\"üéâ Hydrological download complete!\")\n",
        "\n",
        "\n",
        "def download_ndvi_files(start_year, end_year, step_days):\n",
        "    bucket = \"noaa-cdr-ndvi-pds\"\n",
        "    prefix = \"data\"\n",
        "    region = \"us-east-1\"\n",
        "\n",
        "    base_dir = \"data\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        year_dir = os.path.join(base_dir, str(year))\n",
        "        os.makedirs(year_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"\\nüìÖ Listing files for year {year}...\")\n",
        "        cmd = [\n",
        "            \"aws\", \"s3\", \"ls\", f\"s3://{bucket}/{prefix}/{year}/\",\n",
        "            \"--no-sign-request\", \"--region\", region\n",
        "        ]\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"‚ö†Ô∏è  Could not list files for {year}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        lines = result.stdout.strip().split(\"\\n\")\n",
        "        files = [line.split()[-1] for line in lines if line.strip().endswith(\".nc\")]\n",
        "\n",
        "        if not files:\n",
        "            print(f\"‚ö†Ô∏è  No .nc files found for {year}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Found {len(files)} files for {year}\")\n",
        "\n",
        "        # Sort chronologically (they usually include YYYYMMDD in filename)\n",
        "        files.sort()\n",
        "\n",
        "        for i, fname in enumerate(files[::step_days]):  # sample every Nth file\n",
        "            s3_path = f\"s3://{bucket}/{prefix}/{year}/{fname}\"\n",
        "            local_path = os.path.join(year_dir, fname)\n",
        "\n",
        "            if os.path.exists(local_path):\n",
        "                print(f\"‚úÖ Already downloaded: {fname}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"‚¨áÔ∏è  Downloading: {fname}\")\n",
        "            subprocess.run([\n",
        "                \"aws\", \"s3\", \"cp\", s3_path, local_path,\n",
        "                \"--no-sign-request\", \"--region\", region\n",
        "            ])\n",
        "\n",
        "    print(\"üéâ Done! All available NDVI files downloaded.\")\n",
        "\n",
        "\n",
        "\n",
        "# loads data and stores data features\n",
        "def load_raw_noaa_data() -> torch.Tensor:\n",
        "  download_hydrological_files(start_year, end_year)\n",
        "  download_ndvi_files(start_year, end_year)\n",
        "  # process the files into a tensor: not sure what the raw format of each file is.\n",
        "  # TODO: download one year's worth of files ot determine basic format and convert to tensor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: DECIDE IF DATALOADERS ARE A GOOD IDEA BASED ON WHAT TORCH.NN.LSTM TAKES IN\n",
        "def create_dataloaders(raw_tensor, batch_size=32) -> (torch.DataLoader, torch.DataLoader, torch.DataSet):\n",
        "  pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h8HVgWBnDnBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defines the model\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicLSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.model = nn.Sequential((self.lstm, self.fc))\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "    def train(train_loader, epochs=10):\n",
        "      losses = []\n",
        "      for epoch in epochs:\n",
        "        running_loss = 0\n",
        "        for i, data in enumerate(train_loader):\n",
        "          inputs, labels = data\n",
        "          self.optim.zero_grad()\n",
        "\n",
        "          preds = self.model(inputs)\n",
        "          loss = self.loss_fn(pred, labels)\n",
        "          loss.backward()\n",
        "\n",
        "          self.optim.step()\n",
        "          running_loss += loss.item()\n",
        "        losses.append(running_loss)\n",
        "      return losses\n",
        "\n",
        "def test(test_data):\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "851KIS4VEEzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actually call all the code!\n",
        "# HYPERPARAMETERS (to tune)\n",
        "batch_size = 32\n",
        "hidden_size = 64 # idk\n",
        "lr = 0.001\n",
        "epoch_count = 10\n",
        "\n",
        "raw_tensor = load_raw_noaa_data()\n",
        "train_loader, val_loader, test_set = create_dataloaders(raw_tensor, batch_size=32)\n",
        "\n",
        "# DATA FEATURES\n",
        "vector_size, num_weeks = raw_tensor.shape()\n",
        "input_size = vector_size - 1\n",
        "\n",
        "model = BasicLSTM(input_size=input_size, hidden_size=hidden_size)\n",
        "model.train(train_loader)\n",
        "model.test(test_set)"
      ],
      "metadata": {
        "id": "P79-UwFmEBjg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}